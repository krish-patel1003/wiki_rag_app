{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain tiktoken matplotlib seaborn tqdm bs4 requests uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wiki data\n",
    "url = 'https://en.wikipedia.org/wiki/Luke_Skywalker'\n",
    "text_path = 'luke_skywalker_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper function\n",
    "def scrape_wiki_page(url, output_file):\n",
    "    \"\"\"\n",
    "    Scrapes a Wikipedia page, removes links and references, and saves the text.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the Wikipedia page to scrape.\n",
    "        output_file: The path to the output text file.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all headings and paragraphs\n",
    "    text_blocks = soup.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p'])\n",
    "\n",
    "    # Clean each block\n",
    "    cleaned_text = []\n",
    "    for block in text_blocks:\n",
    "        # Remove internal links\n",
    "        for link in block.find_all('a'):\n",
    "            link.replace_with(link.text)\n",
    "        # Remove references section entirely\n",
    "        if block.name == 'sup':\n",
    "            block.extract()\n",
    "        cleaned_text.append(block.text.strip())\n",
    "    \n",
    "    # remove references like example [13] or [citation needed] or [c]\n",
    "    cleaned_text = [re.sub(r'\\[\\d+\\]', '', block) for block in cleaned_text]\n",
    "\n",
    "    # Join cleaned blocks and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(cleaned_text))\n",
    "\n",
    "    cleaned_text = '\\n'.join(cleaned_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape wiki page\n",
    "cleaned_text = scrape_wiki_page(url, text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6682"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find encoding for model\n",
    "# encoding = tokenizer.get_encoding(\"\")  Only works for models from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(cleaned_text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chunks)):\n",
    "    print(tiktoken_len(chunks[i]), chunks[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = uuid.uuid4().hex\n",
    "data = [\n",
    "    {\n",
    "        'id': f'{uid}-{i}',\n",
    "        'text': chunk,\n",
    "        'source': url\n",
    "    } for i, chunk in enumerate(chunks)\n",
    "]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector DB\n",
    "# import faiss\n",
    "\n",
    "# index = faiss.IndexFlatL2(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input User Prompt\n",
    "query = \"Who is Luke Skywalker?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LLM Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM API Call\n",
    "def query_llm(prompt):\n",
    "    url = \"https://api.together.xyz/v1/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3-8b-chat-hf\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 4000,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer 551774149a5aa92cf7b8cee1a296851f6b04cf2a3b4f212317083155ceef6f72\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    print(response.text)\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "response = query_llm(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
